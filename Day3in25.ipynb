{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day3in25.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMDWZnd/LZOzRWa8oJyedYc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReemOmer/25daysofMLDS/blob/main/Day3in25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI2ev4MEATuk"
      },
      "source": [
        "## NLP TensorFlow Course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wLbkL0wsllg"
      },
      "source": [
        "The words have to be represented in a way that computer understands it (Tokenization Process), for example, using number like in ASCII.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFmXW7oQAjsk"
      },
      "source": [
        "Encoding sentences could be done by assigning numbers to words, the same word in a different sentence will have the number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqLUtxsNEvQS"
      },
      "source": [
        "### Tokenization: Words to Numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27Rqnfo-sf6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad0d3139-9d9a-451b-8f77-3f3d434a83a5"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# importing tokenizer api\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "             'Test if for the first time!',\n",
        "             'I shall pass the test!!'\n",
        "]\n",
        "\n",
        "# create object tokenizer, num_words => number of words to tokenize\n",
        "# If the text is longer than num_words, it will select the frequent ones\n",
        "tokenizer = Tokenizer(num_words = 10)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# the list of words after tokenization\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "#the marks will be filtered out\n",
        "print(word_index)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'test': 1, 'the': 2, 'if': 3, 'for': 4, 'first': 5, 'time': 6, 'i': 7, 'shall': 8, 'pass': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA7kEcO_E2uE"
      },
      "source": [
        "### Sequencing: Sentences to Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e20w9MMMECDG",
        "outputId": "34eb9cbc-9fbc-45af-bc18-ba9e756e222c"
      },
      "source": [
        "# convert sentences into list of number\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "print(sequences)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 3, 4, 2, 5, 6], [7, 8, 9, 2, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu1yVrqoFzz5",
        "outputId": "6167f8a8-4bf0-4404-d02d-f5b03eb5aecc"
      },
      "source": [
        "# in case new words appear in sentences and sequenced, only existed words in corpus will be sequenced.\n",
        "test_data = ['The weather is really hot!']\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(test_seq)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PQE5N5DG_D3",
        "outputId": "c6c06fc0-cc70-4610-b030-34cf3cb30359"
      },
      "source": [
        "# to avoid skipping some words, use OOV (Out Of Vocabulary) property\n",
        "tokenizer = Tokenizer(num_words= 10, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(test_seq)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'test': 2, 'the': 3, 'if': 4, 'for': 5, 'first': 6, 'time': 7, 'i': 8, 'shall': 9, 'pass': 10}\n",
            "[[2, 4, 5, 3, 6, 7], [8, 9, 1, 3, 2]]\n",
            "[[3, 1, 1, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksbn6-CdKG2Z"
      },
      "source": [
        "### Padding: Unify Sentences Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOAFDV7uI5Vw",
        "outputId": "a54221f6-6d2c-43bd-caf9-548fd8fdaffa"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Padding adds zeros to sentences at the begining to match the longest sentence\n",
        "# padding='post' sets the zeros at the end\n",
        "padded = pad_sequences(sequences)\n",
        "post_padded = pad_sequences(sequences, padding='post', maxlen=3, truncating='pre')\n",
        "# maxlen=3 if we want the sentences to have fixed length, we can also specify the truncation as pre or post\n",
        "# using truncating='post' \n",
        "print(padded)\n",
        "print(post_padded)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 4 5 3 6 7]\n",
            " [0 8 9 1 3 2]]\n",
            "[[3 6 7]\n",
            " [1 3 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk_-DZYJ0aXR"
      },
      "source": [
        "### Classifier to Recognize sentiment in text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json -O ./sarcasm.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAQdD4J2LX7h",
        "outputId": "994bbae4-cadb-47f3-c2c6-bc5c15a99ec7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-10 08:49:24--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.45.112, 172.217.13.80, 172.217.13.240, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.45.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5643545 (5.4M) [application/json]\n",
            "Saving to: ‘./sarcasm.json’\n",
            "\n",
            "\r./sarcasm.json        0%[                    ]       0  --.-KB/s               \r./sarcasm.json      100%[===================>]   5.38M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2022-01-10 08:49:24 (76.0 MB/s) - ‘./sarcasm.json’ saved [5643545/5643545]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw6Mt7srKW0Y"
      },
      "source": [
        "# the data is stored in json format, we have to change it into python format using json library\n",
        "\n",
        "import json\n",
        "\n",
        "# load the file\n",
        "with open('./sarcasm.json','r') as f:\n",
        "  datastore = json.load(f)\n",
        "\n",
        "  sentences = []\n",
        "  labels = []\n",
        "  urls = []\n",
        "\n",
        "# load the values while iterating through the file\n",
        "  for item in datastore:\n",
        "    sentences.append(item['headline'])\n",
        "    labels.append(item['is_sarcastic'])\n",
        "    urls.append(item['article_link'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9VOT3yn2j_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c9f440-1fe1-45a1-bfeb-a469edbb75f6"
      },
      "source": [
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded = pad_sequences(sequences, padding='post')\n",
        "print(padded[0])\n",
        "print(padded.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  308 15115   679  3337  2298    48   382  2576 15116     6  2577  8434\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n",
            "(26709, 40)\n"
          ]
        }
      ]
    }
  ]
}